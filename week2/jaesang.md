# Kaggle × Google 5-Day AI Agents Intensive 정리

지금 참여 중인 5-Day AI Agents Intensive를 따라가면서  
각 Day마다 무슨 개념을 배웠고, 코드랩에서 뭘 만들었는지, 내가 이해한 포인트 위주로 정리했다.

전체 구성은 이런 패턴을 반복한다.

- 이론
  - 짧은 팟캐스트
  - 화이트페이퍼
- 실습
  - Kaggle 코드랩(노트북 기반) 2개 정도
- 질의응답
  - 라이브 또는 디스코드 Q&A

5일 동안 다루는 큰 주제는 다음과 같다.

1. Introduction to Agents
2. Agent Tools & MCP (Model Context Protocol)
3. Context Engineering: Sessions & Memory
4. Agent Quality (Observability & Evaluation)
5. Prototype to Production (A2A, Agent Engine 배포)

## Day 1 – Introduction to Agents

### 1) 내가 이해한 에이전트 정의

Day 1 화이트페이퍼 Introduction to Agents에서의 에이전트 구조는 다음과 같다.

- LLM = 뇌
- Tools / API = 손, 도구
- Orchestration = 신경계, 워크플로우
- 배포 환경 = 몸, 다리(실제 서비스로 동작하는 틀)

즉 에이전트는 단순히 답변만 하는 LLM이 아니라  
목표를 가지고 툴을 사용하며 환경에 실제로 영향을 주는 자율적인 시스템이라는 관점이다.

또한 에이전트 시스템을 단계별로 설명한다.

- 단일 Task Solver
- 여러 서비스를 연결하는 Connected Solver
- 여러 에이전트가 협업하는 Multi-Agent System

으로 점점 복잡도가 올라가는 구조를 다룬다.

### 2) 코드랩 – 첫 에이전트와 멀티 에이전트

Day 1에서 진행한 실습은 크게 두 가지다.

1. Gemini + ADK로 첫 에이전트 만들기

   - ADK 설치 및 API 키 설정
   - 에이전트 인스트럭션(역할, 목표) 정의
   - 간단한 툴을 붙여서, LLM이 필요할 때 툴을 호출하도록 구성
   - 예: 간단한 도움 에이전트, 계산이나 정보 조회를 툴로 처리

2. 첫 멀티 에이전트 워크플로우
   - 예시 역할 분리
     - Planner Agent: 대략적인 계획 세우기
     - Writer Agent: 실제 텍스트 작성
     - Editor Agent: 검토 및 수정
   - Sequential 패턴으로 에이전트 간 바통을 넘기도록 구성
   - 멀티 에이전트 구조를 통해 한 에이전트에 모든 역할을 몰아넣지 않고 책임을 분리하는 연습

### 3) Day 1 개인 한 줄 정리

프롬프트 잘 쓰기에서 끝나는 것이 아니라  
여러 역할을 가진 에이전트들을 설계하는 것이 중요하다는 느낌을 받았다.

---

## Day 2 – Agent Tools & Interoperability with MCP

### 1) 툴과 MCP에 대한 이해

Day 2 화이트페이퍼는 Agent Tools & Interoperability with Model Context Protocol(MCP)이다.

핵심 포인트는 다음과 같다.

- LLM만 있으면 텍스트로 설명만 할 뿐 실제 세계에서 뭔가를 바꾸지는 못한다.
- 에이전트는 툴을 통해 다음과 같은 일을 할 수 있다.
  - API 호출
  - 데이터베이스 조회
  - 파일 입출력
  - 다른 에이전트 호출

툴은 크게 두 종류로 볼 수 있다.

- Custom Tools
  - 직접 만든 함수, 사내 시스템 API, 다른 에이전트를 래핑한 툴
- Built-in Tools
  - ADK에서 기본 제공하는 HTTP 요청, 파일, 검색 등 공통 유틸리티 툴

MCP(Model Context Protocol)가 중요한 이유는 다음과 같다.

- Agent와 여러 서비스, 툴 사이의 인터페이스를 표준화한다.
- MCP 규격으로 한 번 만들어 두면 다른 모델, 다른 환경에서도 재사용하기 쉽다.
- 장기 실행 작업, 스트리밍, 보안 정책 등을 일정한 방식으로 처리할 수 있다.

또 하나 인상적이었던 부분은 Human-in-the-Loop 패턴이다.

- 결제, 중요한 시스템 수정 같은 고위험 작업은 반드시 "Pause → 사람 승인 → Resume" 구조로 설계하라고 강조한다.

에이전트의 힘은 결국 툴 설계에서 나온다.  
어떤 기능을 툴로 분리해 둘지가 아키텍처의 핵심이라는 점을 느꼈다.

---

## Day 3 – Context Engineering: Sessions & Memory

### 1) 세션과 메모리 개념 정리

Day 3 화이트페이퍼는 Context Engineering: Sessions & Memory이다.

핵심 개념은 다음과 같다.

- Session (단기 컨텍스트)
  - 한 번의 연속된 대화를 담는 컨테이너
  - 내부 구성 요소
    - Events: 유저 메시지, 에이전트 응답, 툴 호출 및 결과 등
    - State: 사용자 이름, 언어, 국가, 선호 설정 등 핵심 키-값 정보
- Memory (장기 컨텍스트)
  - 여러 세션에 걸쳐 유지되는 장기 저장소
  - 예: 사용자 취향, 과거 대화 요약, 업로드한 문서에서 추출한 지식 등

중요한 포인트는 다음과 같다.

- LLM 자체는 원래 stateless하다.
- 세션과 메모리 레이어를 설계해 줌으로써 사용자 입장에서는 기억력 좋은 에이전트처럼 느끼게 만들 수 있다.

긴 히스토리를 그대로 프롬프트에 넣을 수 없기 때문에 요약, 압축, 캐싱 같은 컨텍스트 엔지니어링 기법들도 같이 다룬다.

### 2) 실습 - 상태 있는 에이전트 만들기

Day 3 실습에서 했던 내용은 다음과 같다.

1. 세션 기반 컨텍스트 관리

   - 같은 사용자가 여러 턴에 걸쳐 질문해도  
     이전 대화와 상태를 세션에서 불러와 답변에 반영
   - 예: "어제 만들어준 계획에서 3번만 바꿔줘" 같은 요청을 처리할 때  
     세션 이벤트를 찾아서 3번이 무엇이었는지 확인

2. 메모리로 장기 개인화
   - 사용자 정보와 선호를 메모리에 저장해 두고
   - 다른 세션에서도 그대로 활용
   - 예: 사용자가 "항상 요약부터 보여달라"는 취향을 밝히면  
     이후 세션에서도 요약을 먼저 보여주는 패턴

---

## Day 4 – Agent Quality (Observability & Evaluation)

### 1) Agent Observability

Day 4 화이트페이퍼 Agent Quality에서는 에이전트를 만드는 것보다 운영하면서 개선하는 것에 초점을 둔다.

관측(Observability)은 크게 세 가지 축으로 설명된다.

- Logs
  - 어떤 요청이 들어왔고 어떤 응답을 했는지
  - 어떤 툴을 어떤 파라미터로 호출했는지
- Traces
  - 에이전트가 어떤 경로로 결정을 내렸는지
  - 워크플로우 상에서 어떤 스텝들이 실행되었는지
- Metrics
  - 응답 시간
  - 성공률, 에러율
  - 품질 점수 등 수치 지표

이런 정보가 있어야 에이전트가 왜 이상하게 행동했는지 역추적이 가능하다.

### 2) Agent Evaluation

LLM과 에이전트 출력은 확률적이라  
기존 유닛 테스트 방식으로 평가하기 어렵다는 문제를 다룬다.

이에 대한 대안으로 다음과 같은 방식을 소개한다.

- LLM-as-a-judge
  - 별도의 LLM을 심판으로 사용
  - 응답의 정확성, 안전성, 정책 준수 여부 등을 평가
- 골든 데이터셋 기반 평가
  - 정답이 있는 테스트 셋을 만들어 두고
  - 모델 버전이 바뀔 때마다 성능을 비교

또한 Human Feedback과 Human-in-the-Loop도 품질 관리에 중요한 요소로 등장한다.

### 3) 실습 – 관측과 평가 도입

Day 4 실습에서 진행한 내용은 다음과 같다.

1. Agent Observability 적용

   - 에이전트가 어떤 프롬프트를 LLM에 보냈는지
   - 어떤 툴을 어떤 파라미터로 사용했는지
   - 어디서 에러가 발생했는지를 기록
   - 이를 기반으로 디버깅과 개선 포인트를 찾는 연습

2. Agent Evaluation 파이프라인 구성
   - 간단한 평가 데이터셋을 만들고
   - LLM-as-judge 또는 규칙 기반 평가 로직으로 자동 채점
   - 모델 또는 에이전트 버전을 바꿨을 때 성능 변화 비교

만들었다에서 끝나는 것이 아니라 로그, 트레이스, 지표로 관리되는 에이전트가 진짜 프로덕션용이라는 생각이 들었다.
이게 참 어려운 것 같다. 확률 기반으로 동작하는 AI 를 어떻게 평가하면 좋을지...

---

## Day 5 – Prototype to Production (A2A, Agent Engine 배포)

### 1) A2A(Agent-to-Agent) 개념

마지막 날 화이트페이퍼 Prototype to Production에서는 A2A(Agent-to-Agent) 프로토콜과 실제 배포 흐름을 다룬다.

핵심 포인트는 다음과 같다.

- 여러 팀, 여러 회사에서 만든 에이전트라도
- A2A라는 공통 프로토콜을 사용하면
- 서로 메시지를 주고받으며 협업할 수 있다.

장기적으로는 에이전트 생태계 전체를 염두에 둔 설계라는 느낌이다.

### 2) 프로토타입에서 프로덕션까지의 흐름

Day 5에서 강조하는 전체 라이프사이클은 대략 다음과 같다.

1. 로컬 환경에서 ADK로 에이전트 설계 및 구현
2. ADK CLI를 사용해 Vertex AI Agent Engine에 배포
3. Python SDK나 HTTP API로 실제 서비스에서 호출
4. Cloud 콘솔에서 로그, 지표, 버전 관리

이 단계까지 가면 단순 데모 수준을 넘어 실제 서비스에 올릴 수 있는 에이전트 구조가 갖춰진다.

### 3) 실습 – A2A와 Agent Engine 배포

Day 5 실습에서 진행한 내용은 다음과 같다.

1. A2A 프로토콜로 에이전트 간 상호작용 구현

   - 예: Research Agent, Planner Agent, Executor Agent를 나눠 구성
   - 서로 메시지를 주고받으며 플랜을 세우고 실행하는 흐름 구축

2. Agent Engine 배포
   - 에이전트를 Agent Engine에 올리고
   - 외부에서 호출 가능한 엔드포인트 형태로 제공
   - 배포 후 응답 속도, 에러율 등을 모니터링

---
