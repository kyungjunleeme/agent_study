
# Chapter 1. 생성형 AI의 기본

## 생성형 AI 정의
**생성형 AI(Generative AI)**란 기존의 데이터를 단순히 분류하거나 예측하는 것이 아니라,  
새로운 데이터를 직접 만들어내는 인공지능 기술을 말한다.

예를 들어,  
- 그림을 학습한 모델이 새로운 그림을 그릴 수 있고,  
- 음악 데이터를 학습한 모델이 새로운 멜로디를 작곡하며,  
- 언어를 학습한 모델이 자연스러운 문장을 생성할 수 있다.  

이러한 모델은 **학습 데이터의 확률 분포를 이해하고**,  
그 분포에서 **새로운 샘플을 생성하는 방식**으로 작동한다.

---

## 생성형 AI의 주요 발전 단계

### 1. 통계 기반 기법 (초기 세대)
- **가우시안 혼합 모델(GMM)**, **히든 마르코프 모델(HMM)** 등 통계적 접근법.
- 데이터 분포를 수학적으로 근사하지만,  
  고차원 복잡한 데이터(이미지·음성 등)는 제대로 표현하기 어려움.

### 2. 딥러닝 기반 생성 모델의 등장 (전환점)
딥러닝 기술의 발전으로 **비선형적이고 복잡한 데이터 구조**까지 학습 가능해짐.  
대표적인 모델이 **VAE(Variational AutoEncoder)**와 **GAN(Generative Adversarial Network)**.

---

## 1. 변분 오토인코더 (VAE)
데이터를 잠재 공간(latent space)으로 압축하고, 그 공간에서 다시 데이터를 복원하는 방식.

- **핵심 개념**:  
  데이터 \(x\)를 잠재 변수 \(z\)로 변환(인코딩)하고,  
  다시 \(z\)로부터 \(x'\)를 복원(디코딩)하는 확률적 매핑 학습.

- **비유**:  
  → **VAE**: 복잡한 풍경을 간단한 스케치로 요약했다가, 다시 그림으로 되살리는 화가.  
  → **β-VAE**: 단순 복제 대신 ‘형태·색·스타일’을 분리해 이해하도록 훈련된 미술 전공생.  
  → **CVAE**: “봄 분위기로 그려줘” 같은 조건을 반영해 스타일을 지정할 수 있는 화가.

- **활용 예시**:  
  얼굴 이미지 압축 및 복원, 데이터 차원 축소, 이상치 탐지.

---

## 2. 적대적 생성 신경망 (GAN)
두 신경망(생성기와 판별기)이 서로 경쟁(Adversarial)하며 학습하는 구조.

- **생성기(Generator)**: 가짜 데이터를 만들어 진짜처럼 보이게 함.  
- **판별기(Discriminator)**: 진짜와 가짜를 구분하려고 학습함.  
이 둘이 반복적으로 경쟁하면서 생성기의 품질이 점점 향상된다.

- **비유**:  
  위조지폐범(생성기)과 수사관(판별기)이 경쟁하며  
  서로의 수준을 높이는 게임.

- **주요 변형**:  
  - **DCGAN**: CNN을 이용해 고해상도 이미지 생성.  
    → 스케치북에서 디지털 아트 툴로 진화한 화가.  
  - **WGAN**: Wasserstein 거리를 활용해 학습 안정성과 품질 개선.  
    → 단순히 “가짜다”가 아니라 “얼마나 진짜에 가까운지” 세밀하게 평가.

- **활용 예시**:  
  인물 사진 합성, X-ray 데이터 보강, 스타일 변환(StyleGAN), 이미지 복원.

---

## 3. 자기회귀 모델 (Autoregressive Model)
데이터를 **순차적으로 하나씩 생성**하는 방식.  
문장 생성처럼 순서가 중요한 작업에 특히 강력하다.

### 주요 구성요소
- **셀프 어텐션(Self-Attention)**: 입력 전체 중 중요한 부분에 집중.
- **멀티헤드 어텐션(Multi-Head)**: 여러 관점에서 문맥을 해석.
- **위치 인코딩(Positional Encoding)**: 순서 정보를 모델에 제공.
- **피드포워드 네트워크**: 어텐션 결과를 비선형적으로 변환.
- **잔차 연결 & 정규화**: 안정적 학습과 정보 손실 방지.

### 대표 모델
- **PixelCNN**, **PixelSNAIL** (이미지 순차 생성)
- **GPT, BERT, T5** (텍스트 생성 및 이해)

---

## 대형 언어 모델 (LLM: Large Language Model)
트랜스포머 아키텍처 기반으로,  
인터넷의 대규모 텍스트를 학습해 인간처럼 언어를 이해하고 생성한다.

### 유형별 구분

| 유형 | 특징 | 대표 모델 | 주요 활용 |
|------|------|------------|------------|
| **자기회귀형** | 앞의 단어로 다음 단어 예측 | GPT-3, GPT-4, PaLM | 문장 생성, 자동 완성 |
| **인코더 전용형** | 문맥 이해·분류에 특화 | BERT, RoBERTa | 감정 분석, 분류, 개체명 인식 |
| **인코더-디코더형** | 입력을 이해하고 새로운 출력 생성 | T5, BART | 번역, 요약, 질의응답 |
| **멀티모달형** | 텍스트+이미지+음성 등 융합 처리 | GPT-4, DALL·E, Flamingo, Llava | 이미지 생성, 시각 질의응답 |
| **지시 조정형** | 사용자 프롬프트에 따라 동작 | InstructGPT, ChatGPT | 대화, 도우미 에이전트 |
| **도메인 특화형** | 특정 분야 데이터로 학습 | BioBERT, LegalBERT, FinBERT | 의학, 법률, 금융 분석 |

---

## LLM 기반 AI 에이전트

단순히 텍스트를 생성하는 수준을 넘어,  
**작업 수행 능력(Task-Oriented)**을 갖춘 지능형 시스템으로 발전.

- **특징**
  - 프롬프트를 해석하고 계획을 세움 (Chain-of-Thought)
  - 외부 도구(API, DB, 브라우저 등)와 연동 가능
  - 메모리와 피드백 루프를 통한 자기 개선 가능

- **예시: 항공권 예약 어시스턴트**
  - “주말에 제주도 가는 비행기”처럼 모호한 요청을 받아
  - 필요한 정보를 단계적으로 묻고
  - 가격, 시간, 항공사 옵션을 종합해 추천까지 수행.

---

## 생성형 AI의 응용 분야

| 분야 | 설명 |
|------|------|
| **이미지/비디오 생성** | Stable Diffusion, Midjourney, Runway 등 |
| **텍스트 생성** | GPT, Claude, Gemini |
| **음악·오디오 생성** | Jukebox, Suno, Udio |
| **의료·신약 개발** | 단백질 구조 예측, 화합물 생성 |
| **코드 생성** | Copilot, Codex, Code Llama |
| **로보틱스/워크플로우** | 자율 제어, 작업 자동화 |

---

## 생성형 AI의 과제와 한계

### 1. 데이터 품질과 편향
- 학습 데이터의 불균형은 모델이 편향된 출력을 내게 함.  
  예: 특정 인종/성별/언어의 데이터 과다.
- **해결책**
  - 데이터 다양성 확보  
  - 오버샘플링/언더샘플링 기법  
  - 학습 후 페어니스 검증 및 리밸런싱

---

### 2. 데이터 프라이버시
- 모델이 학습 데이터 일부를 그대로 재생산할 가능성 있음.  
  예: 이메일, 의료 기록 등 민감한 정보 유출.

- **대책**
  - 학습 전 데이터 익명화/가명화  
  - 민감 정보 제거 파이프라인 구축  
  - 프롬프트 인젝션 방어 필터 적용

---

### 3. 계산 자원과 비용
- 대형 모델 학습에는 수백만 달러와 수천 개의 GPU 필요.
- 중소기업/연구소는 **사전학습된 모델 + 파인튜닝**으로 접근.
- 경량화 모델(SLM, LoRA, QLoRA 등)로 효율 향상 가능.

---

### 4. 윤리적·사회적 문제
- **딥페이크**: 얼굴 합성으로 인한 허위 정보 확산  
- **저작권 문제**: 학습 데이터로 사용된 창작물의 권리 이슈  
- **일자리 대체**: 예술·마케팅·고객지원 등 자동화로 인한 구조 변화

---

### 5. 일반화와 창의성의 한계
- 모델은 “훈련 데이터 내에서의 패턴”을 재조합하는 데 강하지만,  
  완전히 새로운 개념을 창조하는 데는 약하다.  
- 진정한 창의성보다는 “확률적으로 가능성 높은 조합”을 택함.

> 즉, 생성형 AI는 뛰어난 모방자이자 조합자이지만,  
> 완전히 새로운 사고를 하는 인간의 창의성에는 아직 미치지 못한다.
>


cf)  LLM의 동작원리

- https://velog.io/@mertyn88/LLM-%EB%8C%80%EA%B7%9C%EB%AA%A8-%EC%96%B8%EC%96%B4-%EB%AA%A8%EB%8D%B8%EC%9D%98-%EC%9E%91%EB%8F%99-%EC%9B%90%EB%A6%AC
- https://newsletter.systemdesign.one/p/llm-concepts?utm_source=post-email-title&publication_id=1511845&post_id=176360129&utm_campaign=email-post-title&isFreemail=true&r=g99jx&triedRedirect=true&utm_medium=email
