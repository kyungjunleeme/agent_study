# Chapter 1. 생성형 AI의 기본

## 생성형AI 정의
생성형 AI는 텍스트, 이미지, 오디오, 비디오 등 다양한 형태의 콘텐츠를 생성할 수 있는 AI 기술들을 의미한다.

학습한 데이터와 유사한 새로운 데이터를 만들어내는 기술
- 확률 분포 학습 -> 샘플링 수행해 새 인스턴스 생성

생성형AI모델
- 기존 데이터를 학습해, 그와 유사하거나 새로운 데이터를 '생성(generate)'할 수 있는 인공지능 모델

통계 기반 기법 중심
- 가우시안 혼합 모델, 히든 마르코프 모델
- 복잡한 데이터 분포의 효과적 모델링에 한계
  딥러닝
- 전환점. VAE, GAN
- VAE: 심층신경망을 활용해 복잡한 데이터 분포 학습 가능
- GAN: 두 개의 뉴럴 네트워크가 게임 이론 바탕으로 경쟁하며 데이터 생성

### 1. VAE
데이터와 잠재 공간(latent space) 사이의 확률적 매핑을 학습하는 것

- vae: 기본 모델. 데이터를 압축하고 재구성. 화가에 비유하자면 정교한 그림을 간단한 스케치로 압축(인코딩)하고, 그 스케치에서 다시 완전한 그림을 복원(디코딩)할 수 있는 능력을 가진 능숙한 화가.
- Beta VAE: 하이퍼파라미터 도입. 화가로 비유해보자면, Beta-VAE는 단순히 그림을 복제하는 것이 아니라 그림의 색상, 형태, 스타일 등을 분해하고 이해하게끔 훈련받은 미술 전공 학생.
- CVAE: 조건 지정 기능 부여. 화가로 다시 비유하자면, CVAE는 지시된 스타일에 맞춰 그림을 그릴 수 있는 화가.

### 2. GAN
GAN은 생성기와 판별기가 서로 경쟁하며 학습하는 신경망 구조로, 생성기는 진짜 같은 데이터를 만들고 판별기는 이를 구별한다. 이 적대적 학습 과정을 통해 생성기는 점점 더 현실적인 데이터를 생성하게 된다.

- GAN은 생성기와 판별기가 서로 경쟁하며 학습해 진짜처럼 보이는 데이터를 만들어내는 기본 모델이다. 위조지폐범(생성기)과 수사관(판별기)이 서로 속이고 잡으려 하는 게임처럼, 경쟁이 반복될수록 더 사실적인 결과를 낸다.
- DCGAN은 여기에 CNN을 결합해 고해상도 이미지를 생성할 수 있도록 한 모델로, 스케치북에서 디지털 아트 스튜디오로 도구가 업그레이드된 화가에 비유할 수 있다. 보다 정밀하고 복잡한 이미지를 사실적으로 표현하는 데 뛰어나다.
- WGAN은 Wasserstein 거리를 손실 함수로 사용해 학습을 안정화하고 품질을 높인 모델이다. 단순히 “좋다/ 나쁘다” 식의 평가 대신, “좀 더 나아지고 있다”처럼 세밀한 피드백이 가능하며, 의료 영상에서 X-ray 이미지 생성 등 실제 AI 학습 데이터 보강에도 활용된다.

## 3. 자기회귀모델 (대표구조: 트랜스포머 아키텍처)
자기회귀 모델은 데이터를 하나씩 순서대로 생성하며, 각 단계의 출력이 다음 단계의 입력에 영향을 준다. 이런 특성 때문에 문장 생성이나 이미지의 순차적 구성처럼 순서가 중요한 작업에 효과적이다.

주요 구성 요소
- 셀프 어텐션 매커니즘: 입력 전체 중 어떤 부분에 집중할지 모델이 스스로 결정해 문맥을 파악한다.
- 멀티 헤드 어텐션: 여러 개의 주의 메커니즘을 동시에 사용해 다양한 시각에서 정보를 해석한다.
- 위치 인코딩: 입력 요소의 순서를 모델이 이해할 수 있도록 위치 정보를 더한다.
- 피드포워드 네트워크: 어텐션 결과를 비선형적으로 변환해 복잡한 표현을 학습한다.
- 레이어 정규화와 잔차 연결: 학습의 안정성을 높이고 정보 손실을 줄인다.

대표 예시
- PixelCNN, PixelSNAIL, GPT, BERT, T5

LLM(대형 언어 모델)
- 트랜스포머 아키텍처를 기반으로 GPT 같은 모델의 성공에서 발전한 개념으로, 인터넷의 대규모 텍스트 데이터를 학습해 인간처럼 텍스트를 이해하고 생성할 수 있다.
- 대표 모델로 GPT-3, GPT-4, PaLM, BERT-large 등이 있으며, 질문 응답, 요약, 번역, 코드 생성 등 다양한 언어 작업에 활용된다.
  주요 유형
- 자기회귀 LLM: 텍스트를 한 단어(토큰)씩 순차적으로 생성하며, 주로 문장 생성과 자동 완성 작업에 사용된다. 예시: GPT-3, GPT-4, PaLM.
- 인코더 전용 LLM: 입력 텍스트의 의미를 분석하는 데 특화되어 있으며, 감정 분석이나 문서 분류, 개체명 인식 등에 활용된다. 예시: BERT, ROBERTa.
- 인코더-디코더 LLM: 텍스트를 이해하면서 동시에 생성할 수도 있어 번역, 요약, 질의응답에 적합하다. 예시: T5, 일부 BERT 기반 모델.
- 멀티모달 LLM: 텍스트뿐 아니라 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 처리·생성할 수 있다. 예시: DALLE, Stable Diffusion, Flamingo, GPT-4, Llava.
- 지시 조정형 LLM: 사용자의 지시(prompt)에 맞춰 동작하도록 미세 조정된 모델로, 특정 작업에 맞게 맞춤형으로 조정된다. 예시: InstructGPT.
- 도메인 특화 LLM: 특정 분야의 전문 데이터를 기반으로 학습되어 해당 분야에 최적화된 모델이다. 예시: BioBERT(의학), LegalBERT(법률), FinBERT(금융).

LLM 기반 AI 에이전트
- 특정 LLM 구조에 국한되지 않고, 여러 LLM 기술을 응용해 구성된 고급 시스템이다.
- 일반적으로 지시 조정형 LLM(InstructGPT 등)을 기반
- 여기에 다양한 기능과 구성 요소를 결합해 특정 목표를 수행하도록 설계된다.
- 이러한 에이전트는 단순한 언어 생성기를 넘어, 사용자의 지시를 이해하고 복잡한 작업을 수행할 수 있는 지능형 시스템으로 발전하고 있다.

항공권 예약 어시스턴트
- 모호한 요청으로 대화 시작해도 예약에 필요한 파라미터에 필요한 정보를 사용자에게 단계별로 요청함.
- 대화 시작부분에 생각의 사슬(chain of thought) 이라 알려진 자기 성찰을 수행.

생성형 AI의 응용
- 이미지 및 비디오 생성
- 텍스트 및 콘텐츠 생성
- 음악 및 오디오 생성
- 의료 및 신약 개발
- 코드 생성
- 자유 워크플로 및 로보틱스

## 생성형 AI의 과제와 한계

데이터 품질과 편향
- 생성형 AI 모델의 성능은 학습 데이터의 품질과 다양성에 크게 좌우된다.
- 이를 완화하기 위해서는 다양한 관점과 배경을 반영한 균형 잡힌 데이터를 확보하고, 데이터의 분포를 사전에 분석해 불균형 여부를 확인하는 과정이 중요하다.
- 또한 오버샘플링이나 언더샘플링 같은 알고리즘적 방법을 활용할 수도 있지만, 데이터마다 효과가 다르기 때문에 신중한 적용이 필요하다.

데이터 프라이버시
- LLM은 학습 과정에서 사용된 데이터를 무의식적으로 재생산해 개인정보나 기업의 독점 정보를 누설할 위험이 있다.
- ex: 프롬프트 인젝션 공격 (모델이 학습 데이터의 일부를 그대로 출력하도록 유도)
- 대책: 훈련 전에 데이터를 익명화하거나 가명화해 민감한 정보를 제거할 수 있음. 다만 이러한 처리 과정이 모델 성능에 영향을 미칠 수 있어, 데이터 보호와 성능 유지 간의 균형 유지 필요.

계산 자원
- 정교한 생성형 AI 모델을 학습하려면 막대한 연산 자원과 비용이 필요하다.
- 대형 언어 모델은 수백만 달러의 연산비용과 고성능 GPU 같은 비싼 하드웨어를 요구하며, 전력 냉각 데이터센터 등 인프라 비용도 상당하다. 이러한 이유로 LLM 학습은 주로 자금력이 있는 대기업이나 연구 기관이 수행한다.
- 그러나 클라우드 서비스를 통해 사전 학습된 모델이나 파인튜닝 기능을 활용하면, 소규모 조직도 LLM 기술을 부분적으로 사용할 수 있다. 또 다른 대안으로는 특정 작업에 맞게 설계된 소형 언어 모델(SLM)이 있으며, 더 적은 자원으로 효율적인 학습이 가능하다.

윤리적, 사회적 함의

- 딥페이크와 허위 정보
- 지적 재산권
- 일자리 대체

일반화와 창의성
- 훈련데이터와 현저히 다른 콘텐츠를 거의 생성하지 못함